Supervised Learning - 지도 학습 - 정답이 있는 데이터를 기반으로 학습, 데이터 분류 / 올바른 결과 예측

1. Regression(회귀) - 연속형 변수 - 변수들 간의 상관관계를 찾는 것, 연속적인(continuous) 데이터로부터 결과를 예측
- 예측 결과가 숫자일 때 사용


2. Classification(분류) - 범주형 변수 - 주어진 데이터를 정해진 범주 (category)에 따라 분류
- 예측 결과가 숫자가 아닐 때 ex) 스펨 메일 필터링, 시험 합격 여부, 재활용 분리수거 품목, 악성 종양 여부


3. Linear Regression(선형 회귀) - 
- Independent variable 독립 변수 (원인) - X(입력 변수, feature) 
- Dependent variable 종속 변수 (결과) - Y(출력 변수, target, label)
- 최적의 선 -  y = mx + b | m : 기울기(slope, coefficient)
				    b : y 절편(intercept)
- 실제 값과 예측 값 차이의 제곱의 합을 최소화 : sum(y0 - y1)^2
										    : 잔차 제곱의 합 : RSS(Residual Sum of Squares) = SSR(Sum of Squared Residuals)
											: 최소제곱법 : OLS(Ordinary Least Squares) = Least Square Method - 잔차 제곱의 합의 최소식을 찾는 법 / 노이즈(이상 값에 벗어난 데이터)에 최약함


4. 데이터 세트 분리 - 훈련 세트(Train set)  	 80 : X_train, y_train
				   - 테스트 세트(Test set) 	    20 : X_test, y_tset


5. 경사 하강법(Gradient Descent) - 학습률을 기반으로 최소의 로스율(기울기 = 0 으로 만드는 것이 목표)을 구함 즉, 최적화하는 방법
								- 많이 쓰이는 학습률(0.001, 0.003, 0.01, 0.03, 0.1, 0.3)
								- 에포크(Epoch) - 최적의 값을 찾기 위해 모든 데이터를 한 번씩 사용하는 방법

5-1. 확률적 경사 하강법(Stochastic Fradient Descent)

6. 다중 선형 회귀(Multiple Linear Regression) - y = b + m1x1 + m2x2 + m3x3 + ..... + mnxn

6-1. 원 핫 인코딩(One-Hot Encoding) - 내가 표현하고자 하는 값 = 1 else = 0 
									- 하는 이유 : 컴퓨터는 문자보다 숫자를 더 잘 처리할 수 있기 때문에 자연어 처리에서 문자를 숫자로 바꾸는 여러가지 기법들을 사용하는대 그 중 하나가 원-핫 인코딩(One-Hot Encoding)이다.
									- 단어 집합(vocabulary)은 서로 다른 단어들의 집합이다. 이 집합의 단어들마다 고유의 번호를 부여한다.
									- 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하고 해당 위치에 1을 부여하고 다른 단어의 인덱스의 위치에는 0을 부여합니다. 



7. 다중공선성(Multicollinearity) - 독립 변수들 간에 서로 강한 상관관계를 가지면서 회귀계수 추정의 오류가 나타나는 문제 : 하나의 피처가 다른 피처에 영향을 줌
								- 다중공선성을 해결법 : Dummy Column이 n개면 n-1개만 사용 -> Dummmy variable tarp

8. 회귀 모델 평가 - MAE(Mean Absolute Error) : 실제 값과 예측 값 차이의 절대값들의 평균
				 - MAE(Mean Squared Error) : 실제 값과 예측 값 차이의 제곱한 값들의 평균
				 - RMSE(Root Mean Squared Error) : MSE에 루트를 적용
				 - 위 세 개는 0에 가까울수록 좋은 모델이다. 상황에 따라 효율적인 식이 될 수 있다.
				 - R^2(R Square) : 결정계수(데이터의 분산을 기반으로 한 평가 지표) : R^2 = 1 - SSE(실제 값 - 예측 값) / SST(실제 값 - 평균 값) = SSR / SST => 1 - (실제 값 - 예측 값)^2의 합 / (실제 값 - 평균 값)^2의 합
				 - 결정계수(R Square)가 크면 좋다.


Unsupercised Learning - 비지도 학습 - 정답이 없는 데이터를 기반으로 학습, 데이터의 유의미한 패턴 / 구조 발견



Reinfocement Learning - 강화 학습 - 상과 벌을 주며 학습, 누적 보상을 최대화 하는 의사결정


